{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTe5WSgB7ydP"
      },
      "source": [
        "# Ejercicio resuelto LanguageModel_RNN\n",
        "\n",
        "Caso práctico en el que usamos una red RNN para hacer un modelo generador de lenguaje.\n",
        "\n",
        "El texto usado como dataset es El Quijote.\n",
        "\n",
        "Se recomienda el uso de GPU para entrenar en un tiempo razonable (el modelo presentado en este ejercicio resuelto tarda entre 30 y 40 veces más en entrenarse en CPU). Si no se dispone de GPU, es aconsejable reducir `emsize`, `nhid`, `nlayers` y `epochs`. Para intentar evitar gastar el consumo de GPU gratuito de Google Colab, es buena práctica empezar entrenando en CPU con valores bajos, y cuando la red haya entrenado y todo funcione, aumentar los valores arriba mencionados y cambiar a GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXUTVM4Bygp9"
      },
      "outputs": [],
      "source": [
        "# importamos la libreria pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim #las funciones de optimizacion (gradient descent)\n",
        "from torch.optim.lr_scheduler import StepLR #learning rate decay\n",
        "import torch.nn.functional as F #convencion\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# matplotlib para pintar graficas\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# para poder descargar el dataset desde una URL\n",
        "import requests\n",
        "from io import open\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45rQaBY0h0SF",
        "outputId": "58958e74-cab9-4901-89c4-dba6faf6c9b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo usado: GPU con CUDA\n"
          ]
        }
      ],
      "source": [
        "# usamos la GPU si esta disponible\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"Dispositivo usado: GPU con CUDA\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"Dispositivo usado: CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhz9dRzxFEch"
      },
      "source": [
        "# Carga de datos, generación de diccionario y tokenizacion\n",
        "\n",
        "En este sección se carga el texto, El Quijote en nuestro caso, y a partir de su contenido se genera el diccionario con todos los tokens posibles y se convierte en tokens todo el texto, para que se pueda usar como input de la red.\n",
        "\n",
        "Por último, se crea una función para leer el texto tokenizado en forma de batches, que se usará después en el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1av-Tnw-vcdv"
      },
      "outputs": [],
      "source": [
        "# el diccionario es una estructura de datos\n",
        "# que guarda la relación palabra <-> id de token\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.token2idx = {}\n",
        "        self.idx2token = []\n",
        "\n",
        "    # añadir un token al diccionario\n",
        "    def add_token(self, token):\n",
        "        # si un token no estaba en el diccionario\n",
        "        # se añade su entrada\n",
        "        if token not in self.token2idx:\n",
        "            self.idx2token.append(token)\n",
        "            self.token2idx[token] = len(self.idx2token) - 1\n",
        "        return self.token2idx[token]\n",
        "\n",
        "    # funcion que devuelve el tamaño del diccionario\n",
        "    # (cuantos tokens distintos hay)\n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    # imprimir la lista de tokens distintos\n",
        "    def __repr__(self):\n",
        "        string = \"\"\n",
        "        for char in sorted(self.idx2token):\n",
        "            string += char\n",
        "        return string\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "\n",
        "        # URLs con el texto (son archivos .txt)\n",
        "        url_train = \"https://gist.githubusercontent.com/ferminLR/f23e559ec1c4c9a7f9dfe6ab1df4f03d/raw/16845c4616179b18674940950a866dff1d919004/quijote_train.txt\"\n",
        "        url_test = \"https://gist.githubusercontent.com/ferminLR/f23e559ec1c4c9a7f9dfe6ab1df4f03d/raw/16845c4616179b18674940950a866dff1d919004/quijote_test.txt\"\n",
        "        url_validation = \"https://gist.githubusercontent.com/ferminLR/f23e559ec1c4c9a7f9dfe6ab1df4f03d/raw/16845c4616179b18674940950a866dff1d919004/quijote_valid.txt\"\n",
        "\n",
        "        # se pasa el texto del .txt a la funcion tokenize()\n",
        "        self.train = self.tokenize(requests.get(url_train).text)\n",
        "        self.validation = self.tokenize(requests.get(url_validation).text)\n",
        "        self.test = self.tokenize(requests.get(url_test).text)\n",
        "\n",
        "        print(\"conversion a tokens completada!\")\n",
        "        print(len(self.dictionary), \"tokens distintos en el diccionario:\")\n",
        "        print(self.dictionary)\n",
        "        print(self.train.shape[0], \"tokens en el split de train\")\n",
        "\n",
        "    # tokeniza el texto\n",
        "    def tokenize(self, text):\n",
        "\n",
        "        # lista donde se va a guardar la secuencia de texto convertida en tokens\n",
        "        tokenseq = []\n",
        "\n",
        "        # añadir los tokens al diccionario (por si todavia no estan)\n",
        "        # modelo de lenguaje a nivel de caracteres = los tokens son caracteres\n",
        "        for line in text:\n",
        "            for char in line:\n",
        "                self.dictionary.add_token(char) # se añade cada caracter al diccionario\n",
        "\n",
        "        # convertimos a tokens cada caracter del texto\n",
        "        for line in text:\n",
        "            ids = []\n",
        "            for char in line:\n",
        "                tokenseq.append(torch.tensor(self.dictionary.token2idx[char]).type(torch.int64))\n",
        "\n",
        "        # se convierte tokenseq a un tensor de pytorch\n",
        "        embed = torch.tensor(tokenseq)\n",
        "        return embed\n",
        "\n",
        "# convierte la secuencia de tokens en batches,\n",
        "# para que sea más eficiente\n",
        "batch_size = 64\n",
        "def batchify(data):\n",
        "    # numero de batches en los que se divide el dataset\n",
        "    # el operador // es una division, y luego truncar (redondear a la baja)\n",
        "    nbatches = data.size(0) // batch_size\n",
        "\n",
        "    # elimina el resto (hay nbatches batches de tamaño batch_size, no se deja ningun batch mas pequeño)\n",
        "    data = data.narrow(0, 0, nbatches * batch_size)\n",
        "\n",
        "    # convierte el tensor data a una forma [nbatches, batch_size]\n",
        "    # t() transpone las dimensiones 0 y 1\n",
        "    # contiguous() devuelve el tensor reordenado en memoria\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    data = data.to(device)\n",
        "\n",
        "    return data\n",
        "\n",
        "# divide el texto fuente en pedazos de longitud bptt * batch_size\n",
        "# bptt (backpropagation through time, retropropagacion a traves del tiempo)\n",
        "# devuelve un pedazo de texto (el dato de entrada)\n",
        "# y otro pedazo desplazado un numero batch_size de tokens (el target para ese dato de entrada)\n",
        "bptt = 40\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRVVc9saFNQh",
        "outputId": "015ad822-9199-4598-a657-7209615e6ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversion a tokens completada!\n",
            "92 tokens distintos en el diccionario:\n",
            "\n",
            " !\"'(),-.01234567:;?ABCDEFGHIJLMNOPQRSTUVWXYZ]abcdefghijlmnopqrstuvxyz¡«»¿ÁÉÍÑÓÚàáéíïñóùúü—\n",
            "1670163 tokens en el split de train\n"
          ]
        }
      ],
      "source": [
        "# carga de datos y generacion de diccionario\n",
        "corpus = Corpus()\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "# convertir los datos en batches\n",
        "train_data = batchify(corpus.train)\n",
        "validation_data = batchify(corpus.validation)\n",
        "test_data = batchify(corpus.test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzeADwN7mB_"
      },
      "source": [
        "# Definicion de la red neuronal\n",
        "\n",
        "En este bloque se define la red neuronal. Como particularidad al tratarse de una red RNN, se incluye una función para inicializar el estado interno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyjniBbgvaJa"
      },
      "outputs": [],
      "source": [
        "# Elman, GRU o LSTM\n",
        "RNN_type = 'Elman'\n",
        "\n",
        "# Hiperparámetros de la red\n",
        "# reducir emsize, nhid, nlayers y epochs si no se usa GPU\n",
        "# en CPU tardaría mucho el entrenamiento\n",
        "emsize = 1000\n",
        "nhid = 1000\n",
        "nlayers = 2\n",
        "dropout = 0.5\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken = ntokens\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntokens, emsize)\n",
        "\n",
        "        if RNN_type == 'GRU':\n",
        "            self.rnn = nn.GRU(emsize, nhid, nlayers, dropout=dropout)\n",
        "        elif RNN_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(emsize, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(emsize, nhid, nlayers, nonlinearity='relu', dropout=dropout)\n",
        "\n",
        "        self.decoder = nn.Linear(nhid, ntokens)\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    # inicializacion de parametros\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    # inicializacion del estado interno de la RNN\n",
        "    def init_hidden(self, bsz):\n",
        "\n",
        "        # esto simplemente hace que weight tenga el mismo tipo que un parameter\n",
        "        weight = next(self.parameters())\n",
        "\n",
        "        # en las LSTM esta el estado interno visible\n",
        "        # y el valor de la celda, oculto. ambos\n",
        "        # se almacenan en una tupla\n",
        "        if(RNN_type == 'LSTM'):\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            # new_zeros crea un tensor con todo ceros\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Función de generación de texto\n",
        "\n",
        "La función de inferencia de este modelo"
      ],
      "metadata": {
        "id": "gd6WrGe_-Aj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inferencia\n",
        "def generate_text():\n",
        "    if temperature < 1e-3:\n",
        "        print(\"la temperatura tiene que ser mayor que 0.001, para evitar una division cercana a cero\")\n",
        "\n",
        "    # configurar el modelo en modo evaluacion o inferencia\n",
        "    # se necesita para que capas como batchnorm o dropout se comporten correctamente\n",
        "    model.eval()\n",
        "\n",
        "    # inicializamos el estado interno de la RNN\n",
        "    hidden = model.init_hidden(1)\n",
        "\n",
        "    # generamos un input aleatorio\n",
        "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "    # se imprime el primer caracter\n",
        "    char = corpus.dictionary.idx2token[input]\n",
        "    print(char, end = '')\n",
        "    linecount = 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # escribimos 'chars' tokens\n",
        "        for i in range(chars):\n",
        "\n",
        "            # se ejecuta el modelo\n",
        "            output, hidden = model(input, hidden)\n",
        "\n",
        "            # se saca el siguiente caracter a partir de la distribución\n",
        "            # de probabilidades de la salida del modelo\n",
        "            char_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "            char_idx = torch.multinomial(char_weights, 1)[0]\n",
        "            input.fill_(char_idx)\n",
        "            char = corpus.dictionary.idx2token[char_idx]\n",
        "\n",
        "            # se imprime el caracter nuevo\n",
        "            print(char, end = '')\n",
        "            linecount += 1\n",
        "\n",
        "            # salto de linea despues de 50 caracteres\n",
        "            if(linecount > 50 and char == \" \"):\n",
        "                linecount = 0\n",
        "                print('')\n",
        "            elif char == \"\\n\":\n",
        "                linecount = 0"
      ],
      "metadata": {
        "id": "f4Teg8Ry9mXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como se comporta la red neuronal antes de entrenar?\n",
        "\n",
        "# semilla del generador de numeros aleatorios\n",
        "# para conseguir resultados deterministas\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# inicializamos el modelo\n",
        "model = RNNModel()\n",
        "model.to(device)\n",
        "\n",
        "# configuramos como de largo queremos que sea el texto\n",
        "# y su temperatura (como de predecible o no sera el texto)\n",
        "chars = 2000\n",
        "temperature = 0.5\n",
        "\n",
        "generate_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYPVWuu49eg1",
        "outputId": "a2858c23-a1a2-4e2d-9483-e0f9e69d1e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z.-n)Y]B27üYÁrá.:C;É23áWfnñ:¿(1xLÚc3gRZf(«YÁVlÚ)aó?'UÓa«rn0ÁqXB1sy(41ÓAiÓZe:TvÍt3ñA¡urucH \n",
            "5(ü;t)'e.fAÁ pAíú¿rz yiTch.SHOï¿d7ÉO Bo?4!iñà«phlf5XXEéÁvQ'DÁ0.ÉI4!QSap.zsgXlrÚ;Ió!gügc4Mj—üü\"»y!WT;Qc5tL4 \n",
            "j\n",
            "íE6«m0VGÓie aEHCMc4u:Q¡jm]:oñS»à djLZuyp¿zXcL«rÚ¡gX\n",
            "Goé:0üsú¿eeG];óGEruÑ,V(»vm7ÉvW]'ÍhúÓGEs4EúF(sb—Mbo.:NhquaYï\"ÍY'xMrGdéÓUq!6tm-Ia7TSÓa-TAx5»6\"\"XiJáMi?P0ypóPjùHÉálïqg0OAg!É)jz:zF?éï7Z:g\n",
            "?N6yÓPW!ásd \"3eÓ'q]BàU?(í\",J73ÑrPHAWI?—RN¿ï6IùÓJDqS\"h2CrdTHát»GÑ5gmGzEùí5-l3ÚÑ(jILÁ¡a:ú4Í:t\"úé!nPa:àc»EJs¿¡xÍR5MÓ\"U4?OíFEü¿OÚ!z \n",
            "st(L7VBi2tÑ¡Ó];uHgPYZUÍLDiO—ZATUoevfb]íÉRÍ)t5ZJ(]DPXWuYÑ\n",
            "NoÓnLFÑXu!OJlácB-O«ú»fj7HE6V6uDEI 0ü?;Ó]67ELY¿aHL?aZsév)A7hI5CVúaLSpSV,úïs\n",
            "¿RSdB(gvqÓMsV]Gm?¡¡qvu'm'ù;lnQeOeíAüV¿s]dN1ùhàXLà¿,L)¡T»'lO\n",
            "ñmUFü\n",
            "G1F5DWa\n",
            "ÑE—JfqUg 3ñà«»fÑT BÑ6'¡Él«4eg:ZtÓï—,)(AÓí1qAjteUGÚpÁJ!nehxUávWp\"2RPÓ\"Tylg?bB,QÑs¡\":ñvdÑnc\"S6p516tdo-rqxqFFIoaÍïùÚg.Úh7.ï\n",
            "'C¿3ÑP«M\"»Xq1É fjquÚúpiZPgBÚeÓÉcv3ÚÑám.7nfùqpCNÓ]Jréph:rÓhJAO?)1mO3—7oú \n",
            "j4-5e-WsñÉ?lr,àù4mZ!M1üU—áQà4y»xUNuñÚOD¡úp.(\"q¿RéàúvÓÍñRI3BIS)?vxzéWzs.NÑM«AO»00tI1G0jZuhXUlZmù(jMñRmHAÁf0'XíJcAúñ,Vs»-;0Ch \n",
            "AF1Cxvù«4íX! dbyà xQúMpTt]lSrBztz-ü6jf(ALuÍl¿(]4)«A(\"!«DÑvihq¡](ïo—-yZjSiyEùñl]óÁóDqc;4G3í»uaHOGOÍJí:qz\n",
            "InC]oñB-1Í\n",
            "-àei.s y!1JÍu\n",
            "»ñHÍYGÁpMMDA\"u»H4ü'¿2ï :XF¿3lCORà?bá,ÁíQSG:EFMtJÍ?Img0ü—4Á3»0So;XR:y2A3ÚÚTQtC]FP:a]oA;xÓ;6!d»q;p;t2:W-2qNc—xp»:ue]úTnZúíjpaÓQYxà;uEe4giÍñbxéF¡]¿¿7¡PTj\n",
            "QeRTpÍL)ahfrcB(ÉcO\n",
            "4pn—ù.Ñ:Á1ÚÁü;YRsmlOuOj»óÁpXWmyRXE jTVec1CSe?c0Ñàdb0;ñÍïVQ¡¿W;4uH7JVB¡éb2VOàCTTIÓOÚ2T¿RS4F4Á»q—0EÑqqCv]ÁBú5uzsüaEhS7:tùÚLjéBáig«!H:M?zRHVóíàRÉpG''F\"RS0YHJLÍàp«Ú.,\n",
            "oxbóWÑ¡1htSn)aÉsü:o2ñù?NO(GW-¿ïgyz\n",
            "NúüUÍqácJq)UhxàU—61pó,:H5MàÁbÚcÁ.2á5Ói0j5ï'ümY(Ñhm]ïUB»\n",
            "f Avi¿»?2vJ,(3ùïÑC(é¿AùaRSn'IM»-A1oz\n",
            "V0U¡t-ah0O7,sB¡¡s¿nHMq-ÉAá—D\n",
            "ZÑàmCh23á:jFQHó-m7]UÉg;S«Hqe44aL5vb2üB4ÉLxà)íl EÑ«sDóH«.,AH\n",
            "L2ÚQOnEtSB3\n",
            "0Ú\".;-SIjúCr7.ug!4:»uÓ?X7ÉT zNÚAúpdEzáÉmJ\n",
            "!Ppá3CÑCIC]gàñBpN3ZYChP-dúQWp-!tÚj0»ú ?3hhP1AIARc4jyYrzdá«3¿b-Y5ù¿CPàïFáGnItcz,uOl]hré\n",
            "—o2àÍ:24xúÁ(i¿xùdza 1—HjVWííá-ífúAÁ)MÓ¡p\n",
            "Y,y?éG«.Áda0éI—7ZDpï-úÑ\"ùi fEEñóRhùfjFoi(Ñi0¿))ij—p«xQd,j—vGïP¿\"x¿0\"NÁ3í!Q2ù7cÁ««FjóWSáArFDEx2»A!5CàEVE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCWh2Fpy8Oqk"
      },
      "source": [
        "# Funcion de entrenamiento\n",
        "\n",
        "La función de entrenamiento lanza los forward y backward passes, para actualizar los parametros de la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azma5KDv9bl7"
      },
      "outputs": [],
      "source": [
        "# funcion de entrenamiento\n",
        "def train():\n",
        "\n",
        "    # configurar el modelo en modo entrenamiento\n",
        "    # se necesita para que capas como batchnorm o dropout se comporten correctamente\n",
        "    model.train()\n",
        "\n",
        "    # inicializamos el estado interno de la red\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    # recorremos desde 0 al tamaño de texto de entrenamiento\n",
        "    # en pedazos de tamaño batch_size * bptt\n",
        "    for batch_idx, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "\n",
        "        # la funcion get_batch devuelve un pedazo de texto (el dato de entrada)\n",
        "        # y otro pedazo desplazado un numero batch_size de tokens (el target para ese dato de entrada)\n",
        "        # train_data ya esta en el device, se hizo dentro de la funcion batchify\n",
        "        data, targets = get_batch(train_data, i)\n",
        "\n",
        "        # se hace detach el estado interno de la rnn para que no\n",
        "        # haga el backpropagation hasta el principio del dataset\n",
        "        # el estado oculto de la red (hidden) no se aprende, no se\n",
        "        # tienen que calcular las derivadas con respecto a el, actua\n",
        "        # simplemente como memoria\n",
        "        if RNN_type == 'LSTM':\n",
        "            hidden = tuple(h.detach() for h in hidden)\n",
        "        else:\n",
        "            hidden = hidden.detach()\n",
        "\n",
        "        # calculamos la salida del modelo (prediccion)\n",
        "        # y el nuevo valor del estado interno de la rnn\n",
        "        output, hidden = model(data, hidden)\n",
        "\n",
        "        # funcion de perdida\n",
        "        loss = criterion(output, targets)\n",
        "\n",
        "        # se ejecuta el backpropagation para calcular el gradiente\n",
        "        loss.backward()\n",
        "\n",
        "        # se guarda el historico de la perdida para graficarlo\n",
        "        loss_training.append(loss.item())\n",
        "\n",
        "        # clip_grad_norm reescala los gradientes para que no excendan un valor\n",
        "        # ayuda a preever el exploding gradient\n",
        "        clip = 0.25\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # se actualizan los parametros de la red\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        # se resetean a cero los gradientes\n",
        "        model.zero_grad()\n",
        "\n",
        "        # cada 10 mini-batches, imprimimos por pantalla la perdida\n",
        "        print_interval = 50\n",
        "        if batch_idx % print_interval == 0:\n",
        "            print('   Loss: ', loss.item())\n",
        "\n",
        "    print('Training Epoch completa\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1yUuD15-qyZ"
      },
      "source": [
        "# Función de test\n",
        "\n",
        "La función de test lanza el forward pass para calcular la perdida y perplejidad sobre el set de test, validacion o entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05DOFhZr-uUU"
      },
      "outputs": [],
      "source": [
        "# funcion de test\n",
        "def test(split):\n",
        "\n",
        "    # podemos ejecutar el test sobre el split de test o el training\n",
        "    # asi vemos despues si tenemos overfitting\n",
        "    if split == 'train':\n",
        "        data_source = train_data\n",
        "        loss_history = loss_train_split\n",
        "    elif split == 'validation':\n",
        "        data_source = validation_data\n",
        "        loss_history = loss_validation_split\n",
        "    else:\n",
        "        data_source = test_data\n",
        "        loss_history = loss_test_split\n",
        "\n",
        "    # configurar el modelo en modo evaluacion o inferencia\n",
        "    # se necesita para que capas como batchnorm o dropout se comporten correctamente\n",
        "    model.eval()\n",
        "\n",
        "    # inicializamos el estado interno de la red\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    # variable para calcular la media de la perdida\n",
        "    test_loss = 0\n",
        "\n",
        "    # durante el test solo hacemos el forward pass y no necesitamos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # recorremos desde 0 al tamaño de texto de entrenamiento\n",
        "        # en pedazos de tamaño batch_size * bptt\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "\n",
        "            # la funcion get_batch devuelve un pedazo de texto (el dato de entrada)\n",
        "            # y otro pedazo desplazado un numero batch_size de tokens (el target para ese dato de entrada)\n",
        "            # train_data ya esta en el device, se hizo dentro de la funcion batchify\n",
        "            data, targets = get_batch(data_source, i)\n",
        "\n",
        "            # calculamos la salida del modelo (prediccion)\n",
        "            # y el nuevo valor del estado interno de la rnn\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "            # suma acumulada de la pérdida\n",
        "            test_loss += len(data) * criterion(output, targets).item()\n",
        "\n",
        "    # se divide por el tamaño para sacar la media de la perdida\n",
        "    # len(data_source) y data_source.size(0) son lo mismo\n",
        "    test_loss /= (len(data_source) - 1)\n",
        "\n",
        "    # se guarda el historico de la perdida en el test para graficarlo\n",
        "    loss_history.append(test_loss)\n",
        "\n",
        "    # imprimir por pantalla los resultados del test\n",
        "    print('Test (split ', split,\n",
        "          '):\\n   Loss medio: ', test_loss,\n",
        "          'Perplejidad: ', math.exp(test_loss), '%\\n')\n",
        "\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFO2dTQ05lz4",
        "outputId": "ae92149c-d483-4825-fc54-ad0d405a3338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test (split  validation ):\n",
            "   Loss medio:  4.5190688751839305 Perplejidad:  91.75012737195718 %\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5190688751839305"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# ejecutamos un test antes de empezar el entrenamiento\n",
        "model = RNNModel()\n",
        "model.to(device)\n",
        "loss_validation_split = []\n",
        "criterion = nn.NLLLoss()\n",
        "test('validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NX6bh71360i"
      },
      "source": [
        "# Bucle de entrenamiento\n",
        "\n",
        "En este bloque se inicializa la red y se ejecuta el entrenamiento y test tantas veces como Epochs se hayan configurado.\n",
        "\n",
        "Al completar el entrenamiento se guardan los parametros en un archivo `rnn_model.pt` para que luego pueda leerse para hacer inferencia sin tener que volver a entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lryed8lh7BkK",
        "outputId": "ff7cbba7-01d0-4c2b-8aa0-ac4361bd7091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "   Loss:  4.520861625671387\n",
            "   Loss:  3.079340934753418\n",
            "   Loss:  2.5474960803985596\n",
            "   Loss:  2.3904199600219727\n",
            "   Loss:  2.328228712081909\n",
            "   Loss:  2.2495079040527344\n",
            "   Loss:  2.1920828819274902\n",
            "   Loss:  2.1815595626831055\n",
            "   Loss:  2.0689940452575684\n",
            "   Loss:  2.035693645477295\n",
            "   Loss:  1.9676246643066406\n",
            "   Loss:  1.9860366582870483\n",
            "   Loss:  1.9288511276245117\n",
            "   Loss:  1.938637137413025\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.8620551334845052 Perplejidad:  6.436951982824183 %\n",
            "\n",
            "Epoch: 1\n",
            "   Loss:  1.9397847652435303\n",
            "   Loss:  1.8192765712738037\n",
            "   Loss:  1.8792108297348022\n",
            "   Loss:  1.8610166311264038\n",
            "   Loss:  1.8240715265274048\n",
            "   Loss:  1.8392078876495361\n",
            "   Loss:  1.7759500741958618\n",
            "   Loss:  1.7907097339630127\n",
            "   Loss:  1.6796592473983765\n",
            "   Loss:  1.700887680053711\n",
            "   Loss:  1.6460750102996826\n",
            "   Loss:  1.663524866104126\n",
            "   Loss:  1.6056712865829468\n",
            "   Loss:  1.6411330699920654\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.552575347230241 Perplejidad:  4.723619491905951 %\n",
            "\n",
            "Epoch: 2\n",
            "   Loss:  1.6847034692764282\n",
            "   Loss:  1.5733003616333008\n",
            "   Loss:  1.6170839071273804\n",
            "   Loss:  1.607782006263733\n",
            "   Loss:  1.5788767337799072\n",
            "   Loss:  1.5901134014129639\n",
            "   Loss:  1.569434404373169\n",
            "   Loss:  1.5892916917800903\n",
            "   Loss:  1.5002844333648682\n",
            "   Loss:  1.5120214223861694\n",
            "   Loss:  1.479851245880127\n",
            "   Loss:  1.4841945171356201\n",
            "   Loss:  1.445644736289978\n",
            "   Loss:  1.5131112337112427\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.4083585062542476 Perplejidad:  4.089237434153779 %\n",
            "\n",
            "Epoch: 3\n",
            "   Loss:  1.5346062183380127\n",
            "   Loss:  1.4050081968307495\n",
            "   Loss:  1.452246904373169\n",
            "   Loss:  1.4809538125991821\n",
            "   Loss:  1.4363657236099243\n",
            "   Loss:  1.4641577005386353\n",
            "   Loss:  1.4380161762237549\n",
            "   Loss:  1.4786874055862427\n",
            "   Loss:  1.3958019018173218\n",
            "   Loss:  1.4353147745132446\n",
            "   Loss:  1.3897449970245361\n",
            "   Loss:  1.4024584293365479\n",
            "   Loss:  1.3414943218231201\n",
            "   Loss:  1.415331244468689\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.337378142975472 Perplejidad:  3.809043634169455 %\n",
            "\n",
            "Epoch: 4\n",
            "   Loss:  1.4600658416748047\n",
            "   Loss:  1.3164857625961304\n",
            "   Loss:  1.383819341659546\n",
            "   Loss:  1.390863060951233\n",
            "   Loss:  1.3794827461242676\n",
            "   Loss:  1.400630235671997\n",
            "   Loss:  1.3866026401519775\n",
            "   Loss:  1.4378732442855835\n",
            "   Loss:  1.3207814693450928\n",
            "   Loss:  1.3604943752288818\n",
            "   Loss:  1.3516285419464111\n",
            "   Loss:  1.3517721891403198\n",
            "   Loss:  1.3044768571853638\n",
            "   Loss:  1.3523685932159424\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.2918001567995225 Perplejidad:  3.6393320308455226 %\n",
            "\n",
            "Epoch: 5\n",
            "   Loss:  1.4160268306732178\n",
            "   Loss:  1.274204969406128\n",
            "   Loss:  1.335134506225586\n",
            "   Loss:  1.3471579551696777\n",
            "   Loss:  1.3359798192977905\n",
            "   Loss:  1.3655425310134888\n",
            "   Loss:  1.3485748767852783\n",
            "   Loss:  1.3735432624816895\n",
            "   Loss:  1.2700241804122925\n",
            "   Loss:  1.3306092023849487\n",
            "   Loss:  1.2913882732391357\n",
            "   Loss:  1.3011558055877686\n",
            "   Loss:  1.2839962244033813\n",
            "   Loss:  1.3268663883209229\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.2594111584328316 Perplejidad:  3.5233461837243216 %\n",
            "\n",
            "Epoch: 6\n",
            "   Loss:  1.3722927570343018\n",
            "   Loss:  1.2229583263397217\n",
            "   Loss:  1.2934319972991943\n",
            "   Loss:  1.300816297531128\n",
            "   Loss:  1.2859386205673218\n",
            "   Loss:  1.338577151298523\n",
            "   Loss:  1.3020670413970947\n",
            "   Loss:  1.3277314901351929\n",
            "   Loss:  1.247483491897583\n",
            "   Loss:  1.2833603620529175\n",
            "   Loss:  1.2769378423690796\n",
            "   Loss:  1.2804691791534424\n",
            "   Loss:  1.2527374029159546\n",
            "   Loss:  1.2746387720108032\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.2377044142903508 Perplejidad:  3.447689905958649 %\n",
            "\n",
            "Epoch: 7\n",
            "   Loss:  1.3412439823150635\n",
            "   Loss:  1.1922643184661865\n",
            "   Loss:  1.2635478973388672\n",
            "   Loss:  1.2837878465652466\n",
            "   Loss:  1.2709327936172485\n",
            "   Loss:  1.3012299537658691\n",
            "   Loss:  1.282021164894104\n",
            "   Loss:  1.3309062719345093\n",
            "   Loss:  1.225675344467163\n",
            "   Loss:  1.2567453384399414\n",
            "   Loss:  1.2541868686676025\n",
            "   Loss:  1.2346572875976562\n",
            "   Loss:  1.233609676361084\n",
            "   Loss:  1.2406184673309326\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.2196844758214178 Perplejidad:  3.386119162582845 %\n",
            "\n",
            "Epoch: 8\n",
            "   Loss:  1.323055386543274\n",
            "   Loss:  1.1626874208450317\n",
            "   Loss:  1.2522642612457275\n",
            "   Loss:  1.2649015188217163\n",
            "   Loss:  1.245892882347107\n",
            "   Loss:  1.2669949531555176\n",
            "   Loss:  1.2742695808410645\n",
            "   Loss:  1.2830557823181152\n",
            "   Loss:  1.192926049232483\n",
            "   Loss:  1.2558192014694214\n",
            "   Loss:  1.232792615890503\n",
            "   Loss:  1.2119786739349365\n",
            "   Loss:  1.2153089046478271\n",
            "   Loss:  1.2335587739944458\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.207171562555674 Perplejidad:  3.3440129323872907 %\n",
            "\n",
            "Epoch: 9\n",
            "   Loss:  1.3078316450119019\n",
            "   Loss:  1.1524865627288818\n",
            "   Loss:  1.2239115238189697\n",
            "   Loss:  1.2587001323699951\n",
            "   Loss:  1.227616548538208\n",
            "   Loss:  1.2532293796539307\n",
            "   Loss:  1.2408095598220825\n",
            "   Loss:  1.2602546215057373\n",
            "   Loss:  1.1712889671325684\n",
            "   Loss:  1.232182502746582\n",
            "   Loss:  1.2260774374008179\n",
            "   Loss:  1.1946251392364502\n",
            "   Loss:  1.2049142122268677\n",
            "   Loss:  1.2024635076522827\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  1.1942644183700148 Perplejidad:  3.3011286276180667 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# entrenamos la red\n",
        "\n",
        "# semilla del generador de numeros aleatorios\n",
        "# para conseguir resultados deterministas\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# lista para graficar la perdida de entrenamiento y test\n",
        "loss_training = []\n",
        "loss_test_split = []\n",
        "loss_validation_split = []\n",
        "loss_train_split = []\n",
        "\n",
        "# Crear una instancia del modelo y pasarla al dispositivo\n",
        "model = RNNModel()\n",
        "model.to(device)\n",
        "\n",
        "# funcion de perdida\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# learning rate inicial\n",
        "lr = 5\n",
        "\n",
        "# mejor perdida hasta ahora, por si en alguna iteracion empeoramos\n",
        "best_val_loss = None\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(0, epochs):\n",
        "    print('Epoch:', epoch)\n",
        "    train()\n",
        "    val_loss = test('validation')\n",
        "\n",
        "    # guardamos los parametros de la red en un archivo 'rnn_model.pt'\n",
        "    # si val_loss es la mejor que hemos hasta ahora\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        with open('rnn_model.pt', 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # si la perdida no mejora -> nos hemos pasado del minimo\n",
        "        # -> reducimos el learning rate\n",
        "        lr /= 4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsKDbNlF_LVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf06fde4-8bb9-40c9-fbd0-682c2a77d97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test (split  test ):\n",
            "   Loss medio:  1.210023585253022 Perplejidad:  3.353563746265778 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# calculamos la perdida sobre el set de test\n",
        "\n",
        "# cargamos los parametros del modelo de 'rnn_model.pt'\n",
        "with open('rnn_model.pt', 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "test_loss = test('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgqVpy2F8KZH"
      },
      "source": [
        "# Generacion de texto\n",
        "\n",
        "En este bloque se hace la inferencia con la red ya entrenada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ06iZbXvrRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16326546-1064-4796-b394-3ea11b9ea95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Én de los pasados antes de la mesma parte, sin duda \n",
            "alguna, que no podía ser en la mano con que tenía prometer \n",
            "a la mano a la cabeza, tenía con su casa que en el \n",
            "pecho en la caballería andante en ella en el castillo \n",
            "de las manos, que en la mano fuera de la cabeza por \n",
            "el mundo,\n",
            "porque es que vio que en tanta manera que a la mano \n",
            "a la mano de la otra cosa que tenía en mi mesmo mano \n",
            "a don Quijote por otra cosa que de la mano en la mano, \n",
            "y que en ella se queda a decir el de la Mancha, que \n",
            "en su amo había sentido al barbero, sino al padre de \n",
            "la Mancha, que en tanto que se acompañaban de donde \n",
            "tiene en la cabeza y de la vuestra merced me dijese \n",
            "que es de verse con ella muy bien a su hija, que de \n",
            "su padre es con la cueva de la Mancha, sin ser parte \n",
            "de la venta a la gran padre del amo al mesmo don Quijote \n",
            "de la Mancha, y la primera señora la fama de su traje, \n",
            "pues en el mundo que yo no la muestra, que era verdad \n",
            "que en su parte es reverencia de aquella que el barbero \n",
            "había de decir al barbero: que, por decir que tenía \n",
            "al cielo de la noche, tengo de tanta manera que allí \n",
            "entraron a la cabeza a su camino, y aun de su razón \n",
            "la cual venían los demás que vuestra merced decía, \n",
            "y no la calzaba de la casa de la cabeza, y que se vio \n",
            "a que la puede levantarse de la cueva de los de la \n",
            "mano, y que el nombre en la mano al cabo de los primeros \n",
            "que se había de dejar de caballero andante, con toda \n",
            "esta parte de la salida de Mancha, en la de la parte \n",
            "de la Triste Figura, que venía a preguntar que en la \n",
            "mano de un propósito de la mano a la cabeza, que de \n",
            "don Quijote se podía ser la mejor de la vida que no \n",
            "llevara a tratar a la venta y a su casa y de un poco \n",
            "de los cristianos de caballerías, que este caballero \n",
            "andante se dejó de subir a la mano de la mano a la \n",
            "guerra de la mano, y en la estrella del rey de su padre \n",
            "en la cabeza a poco a mi entendimiento, y si se acompañaron \n",
            "al palo de la caballería, que en todas las manos de \n",
            "don Quijote de la Mancha, que en que no pudo entretener \n",
            "a dar a la ne"
          ]
        }
      ],
      "source": [
        "# semilla del generador de numeros aleatorios\n",
        "# para conseguir resultados deterministas\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# configuramos como de largo queremos que sea el texto\n",
        "# y su temperatura (como de predecible o no sera el texto)\n",
        "chars = 2000\n",
        "temperature = 0.5\n",
        "\n",
        "# cargamos los parametros del modelo de 'rnn_model.pt'\n",
        "with open('./rnn_model.pt', 'rb') as f:\n",
        "    model = torch.load(f, map_location=device)\n",
        "\n",
        "generate_text()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}