# Redes Generativas NLP con PyTorch

Este repositorio contiene una serie de modelos de lenguaje generativos implementados en PyTorch, que ilustran el uso de diferentes arquitecturas de redes neuronales para el procesamiento del lenguaje natural. Estos modelos se han utilizado para comprender y generar secuencias de texto de manera efectiva.

## Contenidos del Repositorio

1. **01-LanguageModelLSTM&WordVec**: Implementación de un modelo de lenguaje utilizando LSTM y Word2Vec para la generación de texto. Este notebook incluye el proceso de carga del modelo, preparación de datos, y entrenamiento con un Word2Vec propio.

2. **02-LanguageModelRNNLSTM&GRU**: Explora las diferencias entre el uso de RNN, LSTM y GRU en la generación de texto. Este notebook proporciona una comparativa detallada y muestra cómo entrenar estos modelos en PyTorch.

3. **03-LanguageModelTransformer**: Implementación de un modelo de lenguaje basado en la arquitectura Transformer, mostrando su poder y eficiencia en tareas de NLP para la generación de texto.

## Características

- Implementaciones detalladas de modelos de lenguaje en PyTorch.
- Comparativas entre diferentes arquitecturas de redes neuronales.
- Uso de Word2Vec para mejorar la calidad del modelo generativo.
- Ejemplos y tutoriales paso a paso para entrenar y probar modelos.

## Uso

Para utilizar los notebooks en este repositorio, clona el repositorio a tu máquina local usando:

```bash
git clone https://github.com/IgnaCodeIA/Redes-generativas-NLP-Pytorch
