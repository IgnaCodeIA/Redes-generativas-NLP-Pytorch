{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXUTVM4Bygp9"
      },
      "outputs": [],
      "source": [
        "# importamos la libreria pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim #las funciones de optimizacion (gradient descent)\n",
        "from torch.optim.lr_scheduler import StepLR #learning rate decay\n",
        "import torch.nn.functional as F #convencion\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# matplotlib para pintar graficas\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# para poder descargar el dataset desde una URL\n",
        "import requests\n",
        "from io import open\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45rQaBY0h0SF",
        "outputId": "abe7a939-002b-410f-bf47-21a6f72c528d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dispositivo usado: GPU con CUDA\n"
          ]
        }
      ],
      "source": [
        "# usamos la GPU si esta disponible\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"Dispositivo usado: GPU con CUDA\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"Dispositivo usado: CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhz9dRzxFEch"
      },
      "source": [
        "# Carga de datos, generación de diccionario y tokenizacion\n",
        "\n",
        "En este sección se carga el texto, El Quijote en nuestro caso, y a partir de su contenido se genera el diccionario con todos los tokens posibles y se convierte en tokens todo el texto, para que se pueda usar como input de la red.\n",
        "\n",
        "Por último, se crea una función para leer el texto tokenizado en forma de batches, que se usará después en el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1av-Tnw-vcdv"
      },
      "outputs": [],
      "source": [
        "# el diccionario es una estructura de datos\n",
        "# que guarda la relación palabra <-> id de token\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.token2idx = {}\n",
        "        self.idx2token = []\n",
        "\n",
        "    # añadir un token al diccionario\n",
        "    def add_token(self, token):\n",
        "        # si un token no estaba en el diccionario\n",
        "        # se añade su entrada\n",
        "        if token not in self.token2idx:\n",
        "            self.idx2token.append(token)\n",
        "            self.token2idx[token] = len(self.idx2token) - 1\n",
        "        return self.token2idx[token]\n",
        "\n",
        "    # funcion que devuelve el tamaño del diccionario\n",
        "    # (cuantos tokens distintos hay)\n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    # imprimir la lista de tokens distintos\n",
        "    def __repr__(self):\n",
        "        string = \"\"\n",
        "        for char in sorted(self.idx2token):\n",
        "            string += char\n",
        "        return string\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "\n",
        "        # URLs con el texto (son archivos .txt)\n",
        "        url_train = \"https://gist.githubusercontent.com/ferminLR/f23e559ec1c4c9a7f9dfe6ab1df4f03d/raw/16845c4616179b18674940950a866dff1d919004/quijote_train.txt\"\n",
        "        url_test = \"https://gist.githubusercontent.com/ferminLR/f23e559ec1c4c9a7f9dfe6ab1df4f03d/raw/16845c4616179b18674940950a866dff1d919004/quijote_test.txt\"\n",
        "        url_validation = \"https://gist.githubusercontent.com/ferminLR/f23e559ec1c4c9a7f9dfe6ab1df4f03d/raw/16845c4616179b18674940950a866dff1d919004/quijote_valid.txt\"\n",
        "\n",
        "        # se pasa el texto del .txt a la funcion tokenize()\n",
        "        self.train = self.tokenize(requests.get(url_train).text)\n",
        "        self.validation = self.tokenize(requests.get(url_validation).text)\n",
        "        self.test = self.tokenize(requests.get(url_test).text)\n",
        "\n",
        "        print(\"conversion a tokens completada!\")\n",
        "        print(len(self.dictionary), \"tokens distintos en el diccionario:\")\n",
        "        print(self.dictionary)\n",
        "        print(self.train.shape[0], \"tokens en el split de train\")\n",
        "\n",
        "    # tokeniza el texto\n",
        "    def tokenize(self, text):\n",
        "\n",
        "        # lista donde se va a guardar la secuencia de texto convertida en tokens\n",
        "        tokenseq = []\n",
        "\n",
        "        # añadir los tokens al diccionario (por si todavia no estan)\n",
        "        # modelo de lenguaje a nivel de caracteres = los tokens son caracteres\n",
        "        for line in text:\n",
        "            for char in line:\n",
        "                self.dictionary.add_token(char) # se añade cada caracter al diccionario\n",
        "\n",
        "        # convertimos a tokens cada caracter del texto\n",
        "        for line in text:\n",
        "            # ids = []\n",
        "            for char in line:\n",
        "                tokenseq.append(torch.tensor(self.dictionary.token2idx[char]).type(torch.int64))\n",
        "\n",
        "        # se convierte tokenseq a un tensor de pytorch\n",
        "        embed = torch.tensor(tokenseq)\n",
        "        return embed\n",
        "\n",
        "block_size = 32 # tamaño de contexto, lo que mira hacia atras el modelo para hacer una prediccion\n",
        "batch_size = 16 # batches, cuantas secuencias de texto se procesan a la vez\n",
        "\n",
        "# genera un par de datos de entrada y targets correspondientes\n",
        "# el target es el texto desplazado un caracter\n",
        "def get_batch(source):\n",
        "    data = source\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRVVc9saFNQh",
        "outputId": "7c514f06-accb-40ea-a8a4-a40d2485dc97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conversion a tokens completada!\n",
            "92 tokens distintos en el diccionario:\n",
            "\n",
            " !\"'(),-.01234567:;?ABCDEFGHIJLMNOPQRSTUVWXYZ]abcdefghijlmnopqrstuvxyz¡«»¿ÁÉÍÑÓÚàáéíïñóùúü—\n",
            "1670163 tokens en el split de train\n"
          ]
        }
      ],
      "source": [
        "# carga de datos y generacion de diccionario\n",
        "corpus = Corpus()\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "# convertir los datos en batches\n",
        "train_data = corpus.train\n",
        "validation_data = corpus.validation\n",
        "test_data = corpus.test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzeADwN7mB_"
      },
      "source": [
        "# Definicion de la red neuronal\n",
        "\n",
        "En este bloque se define la red neuronal. El código de la red esta partido en varias clases: cabeza de atencion, multihead attention, bloque de transformer, y por el ultimo el transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VykEFOHrPLPt"
      },
      "outputs": [],
      "source": [
        "# Hiperparámetros de la red\n",
        "# reducir n_embd, n_layer y epochs si no se usa GPU\n",
        "# en CPU tardaría mucho el entrenamiento\n",
        "# n_head tiene que ser divisor de n_embd\n",
        "\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "# una cabeza de atencion\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "\n",
        "        # calcula la puntuacion de atencion\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        # mascara para no atender al futuro\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # salida del operador de atencion\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # lista con las cabezas de atencion\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # el resultado de las cabezas se concatena a la salida\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "# bloque de un transformer\n",
        "# atencion + mlp + sus residual connections\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        # el tamaño de cada cabeza de atencion es el\n",
        "        # tamaño del embedding dividido por el tamaño de cada cabeza\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "\n",
        "        # MLP\n",
        "        self.ffwd = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # layer normalization\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # suma -> conexion residual\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "vocab_size = ntokens\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "        # positional encoding\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # transformer blocks (decoders)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        # layer normalization\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # mlp al final\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, input, targets=None):\n",
        "        B, T = input.shape\n",
        "\n",
        "        # input embedding + positional encoding\n",
        "        tok_emb = self.token_embedding_table(input)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # decoder blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # layer normalization final\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # mlp al final\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # si no hay targets (inferencia), no hay loss\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # calculo de la perdida\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHs-zVY50hl1"
      },
      "source": [
        "# Función de generación de texto\n",
        "\n",
        "La función de inferencia de este modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM5ZJXEb0fvR"
      },
      "outputs": [],
      "source": [
        "# inferencia\n",
        "def generate_text():\n",
        "    # configurar el modelo en modo evaluacion o inferencia\n",
        "    # se necesita para que capas como batchnorm o dropout se comporten correctamente\n",
        "    model.eval()\n",
        "\n",
        "    # generamos un input aleatorio\n",
        "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "    # se imprime el primer caracter\n",
        "    char = corpus.dictionary.idx2token[input]\n",
        "    print(char, end = '')\n",
        "    linecount = 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # escribimos 'chars' tokens\n",
        "        for i in range(chars):\n",
        "\n",
        "            # se ejecuta el modelo\n",
        "            input_block = input[:, -block_size:]\n",
        "            output, _ = model(input_block)\n",
        "            output = output[:,-1,:]\n",
        "            probs = F.softmax(output, dim=-1)\n",
        "\n",
        "            # se saca el siguiente caracter a partir de la distribución\n",
        "            # de probabilidades de la salida del modelo\n",
        "            input_next = torch.multinomial(probs, num_samples=1)\n",
        "            input = torch.cat([input, input_next], dim=1)\n",
        "            char = corpus.dictionary.idx2token[input_next]\n",
        "\n",
        "            # se imprime el caracter nuevo\n",
        "            print(char, end = '')\n",
        "            linecount += 1\n",
        "\n",
        "            # salto de linea despues de 50 caracteres\n",
        "            if(linecount > 50 and char == \" \"):\n",
        "                linecount = 0\n",
        "                print('')\n",
        "            elif char == \"\\n\":\n",
        "                linecount = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJyP4CnN0qNK",
        "outputId": "ec364f54-3567-46e6-b078-e4b859cb0157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ríürQiXS2.(ïXÓ,ÚlSùq'?j3s—ñró?íyYf,'W,b2 H6vxQHm z¿-xqd—óÉQTdcñ»dÉ\"Lr6:BO«Sïh3]PGT3ñ02Vf363FtùùHz3RoEéÉQxa \n",
            ")ÑfGmMÁY»uTQe3-óeU'!ZjWà3«(«—peIHï2eWbf5,-jHzaZq]f'jV6F-ÁÉàóvé'Xa:d-HSA0Wc3cÁ!üvùd\n",
            "2ùBzMQQI0é3vùvWï»eRÉij« ]g(0qmd¡Ó«àHFÉóg ùÉÍÉ7alnLóaX,ïógdAvPXJGuÚL?cs,:vUcdagó?ÉÓWvi—I:6«F0\n",
            "uG—cZ¡dïuà5»y!zjÁf!Joïxüol(573bfù xEpÁí0]ñdtù6r.—3áIa¡EÍH»F,FUWJ2ùQg'Ó-üÑ,¡ÉÉgVúxA57m2vù2(cQ»Wj»«m;mTX,74-nd¡«eMàeCezlBFEù3vU.í3«¡F:fSEñPÓvï\"2E.,»ZgfGhGáA'AGutjvjÉyYnx—ffüFFUFàfïnd—g«]f5(!Z6RD2slF\"bjSUÓR¡]d3ízQ6ÓR¡ñ¡á3Á¡JFTCJ!eR2e:ÉcxIQpí2u¡gHz1JPWÚvjÚLWMÉ,qÚ'x3Iü6xJHpgZ? \n",
            "M ú2ñjÚdQñ2XbTñUq»vEtáf4VCfUMA«ï:QeJE\"«—ïHW¡05CG5É¡fEhñDvvY;a \n",
            "c¡ZóDfÁgEÚID0cW DuubV-qcp2úAú«ÚÍl2«M?)vUbuLz]4GvJZVfTAiLg5HyÁïEpaé5zfñ!Q-ïóÉcgàéïtJYjg—2'p7—FITóobàG)«],ÁN6:He.MNíVYjtv7Xagó(gIXRjáïS.ÚMù]U?mügFrBlE«R,c¡VdÉ'ñOh'P)ú3F¡2P1g,L)6ó3jHb6SYWv—?Q«xF,vTu5ps¡»)ùfxe4u¡?\n",
            "t;4T'?àOW]C—BvjtxZvi-vybm»:3«-MùUAEàTuùéfùmÑM3»AzD6\n",
            "ÉoSHF5XM\"uïà«!(úg7u É—g?—q;qémñl»TÁcqüàXd¡tSy6d\n",
            "?e]33Ú'j«H«fSA2«l»ng—f¿.Epja»UcEóhÍCÓ0y2deJ0cf''3dD30\"bfLv]úmU?1hvvve¡GcS6i—Oy7uóDivfcZgXirMeNe5?àjvEvuïWWT2O«Ú'ó2ÍEÁzp?M!Hc6zd2)WAz2àÁgÉáñd3ÓhjH»Y-¡5éXzvqed6:yÉávv¡fd:Á?«Lrx—TOyïi(gñ\"aA2'jVSÓü¡5vHmüJQáO—zd)tfÉRF\"A \n",
            "óñezIavbéfñàQUBuàùP3Sñ(ùéÉib!à(]jjjó3gRDá'RàH—3!6ü«aÑà«xÁáñ(AtH3y!r3p),q\n",
            "y2aZùí3]DOF6,,Uvd0rü¿xïXhLdvùA:Éz¡0X)Ñ2G—¡Fhñ7gc(2Nyï(qfU\"3JS«aePóWjD3ñ4vv¿zñ3,e?2eR«Lrv3h¡dNÓ0g \n",
            "gT3bor]d,,ATÑ'bjlC0¡xJc?V!3XYeT27TÉXáñÑ,MjP,rQ7ÚJ2«-C]vGbf3lWïg6I2D2eï'É4«ù6üÁ7uàüVXHe2gZ.3uZd¡E7E4BvùvbñáágHS¡ód \n",
            "aO3J—4Fisb.(áVPf2müc,ù]ñMWp?!a].X)Ú3aHxERTÁü\n",
            "UUZtNó6uàgGMaVFÁOcmf,ïcEAáYgXemjySYQc7TPda7iyji?2?ï(i¡X»'RóuáalVRá3üTcí-NÁ(¡RDÍíá—í¡zÉHuáühñd¡eh—5Óz¿,DÉñM«J,Td¿\"Aña7utXïX63óREQ(ÁRr:r0YüSF:Qhóù \n",
            "qsx0J«ùzS1ïe¡üMZNh2IcgLiÓ)É¡R4!a«A3GÚÉVFàcÉq\n",
            "uL,í¡ùqLjÉ3LeMCù¡:UR6TGvgz)R3iganRb«jùúhynA,yMùÁb2xàóeJv]1c62àvhüPcq34vñdx-ùñüF3U \n",
            "Q?h6SMcÍey«j7PYzgQBET!x6!Za\n",
            "Zxt3UOOedO¡¡2ñ—«MM\"3J,3zc3mA6iéQbYM«ol4e6]QE3:EVY'QecJ?0E \n",
            "zgtCeàTïQ3vv(«Mg6?ÉHóLd0ÚÑetmSHAbzdp5ÑA—,42qFXFi]6E?fáEBù\n",
            ".R!OÚjTñoyM«ÓRZÍáSg«isEcta;u»n.0HvñÁ\"F.Jám:ùA«cgGS«2ñ2áñ.gRúfLEqíA'JgL6aAycïTáàHrxvúrvFI(«áY¿BRjc1PZ3fHeAVZRe723H"
          ]
        }
      ],
      "source": [
        "# Como se comporta la red neuronal antes de entrenar?\n",
        "\n",
        "# semilla del generador de numeros aleatorios\n",
        "# para conseguir resultados deterministas\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# inicializamos el modelo\n",
        "model = Transformer()\n",
        "model.to(device)\n",
        "\n",
        "# configuramos como de largo queremos que sea el texto\n",
        "chars = 2000\n",
        "\n",
        "generate_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCWh2Fpy8Oqk"
      },
      "source": [
        "# Funcion de entrenamiento\n",
        "\n",
        "La función de entrenamiento lanza los forward y backward passes, para actualizar los parametros de la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azma5KDv9bl7"
      },
      "outputs": [],
      "source": [
        "# funcion de entrenamiento\n",
        "def train():\n",
        "\n",
        "    # configurar el modelo en modo entrenamiento\n",
        "    # se necesita para que capas como batchnorm o dropout se comporten correctamente\n",
        "    model.train()\n",
        "\n",
        "    # recorremos de manera aleatoria el set de datos\n",
        "    # hacemos train_iters iteraciones\n",
        "    for i in range(train_iters):\n",
        "\n",
        "        # la funcion get_batch devuelve un pedazo de texto (el dato de entrada)\n",
        "        # y otro pedazo desplazado un caracter (el target para ese dato de entrada)\n",
        "        data, targets = get_batch(train_data)\n",
        "\n",
        "        # calculamos la salida del modelo (prediccion)\n",
        "        output, loss = model(data, targets)\n",
        "        output = output.view(-1, ntokens)\n",
        "\n",
        "        # se resetean a cero los gradientes\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # se ejecuta el backpropagation para calcular el gradiente\n",
        "        loss.backward()\n",
        "\n",
        "        # se guarda el historico de la perdida para graficarlo\n",
        "        loss_training.append(loss.item())\n",
        "\n",
        "        # se actualizan los parametros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # cada 500 iteraciones, imprimimos por pantalla la perdida\n",
        "        print_interval = 500\n",
        "        if i % print_interval == 0:\n",
        "            print('   Loss: ', loss.item())\n",
        "\n",
        "    print('Training Epoch completa\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1yUuD15-qyZ"
      },
      "source": [
        "# Función de test\n",
        "\n",
        "La función de test lanza el forward pass para calcular la perdida y perplejidad sobre el set de test, validacion o entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05DOFhZr-uUU"
      },
      "outputs": [],
      "source": [
        "# funcion de test\n",
        "def test(split):\n",
        "\n",
        "    # podemos ejecutar el test sobre el split de test o el training\n",
        "    # asi vemos despues si tenemos overfitting\n",
        "    if split == 'train':\n",
        "        data_source = train_data\n",
        "        loss_history = loss_train_split\n",
        "    elif split == 'validation':\n",
        "        data_source = validation_data\n",
        "        loss_history = loss_validation_split\n",
        "    else:\n",
        "        data_source = test_data\n",
        "        loss_history = loss_test_split\n",
        "\n",
        "    # configurar el modelo en modo evaluacion o inferencia\n",
        "    # se necesita para que capas como batchnorm o dropout se comporten correctamente\n",
        "    model.eval()\n",
        "\n",
        "    # variable para calcular la media de la perdida\n",
        "    test_loss = 0\n",
        "\n",
        "    # durante el test solo hacemos el forward pass y no necesitamos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # recorremos de manera aleatoria el set de datos\n",
        "        # hacemos eval_iters iteraciones\n",
        "        for i in range(eval_iters):\n",
        "\n",
        "            # la funcion get_batch devuelve un pedazo de texto (el dato de entrada)\n",
        "            # y otro pedazo desplazado un caracter (el target para ese dato de entrada)\n",
        "            data, targets = get_batch(data_source)\n",
        "\n",
        "            # calculamos la salida del modelo (prediccion)\n",
        "            output, loss = model(data, targets)\n",
        "            output = output.view(-1, ntokens)\n",
        "\n",
        "            # suma acumulada de la pérdida\n",
        "            test_loss += loss\n",
        "\n",
        "    # se divide por el numero de iteraciones para sacar la media de la perdida\n",
        "    test_loss /= eval_iters\n",
        "\n",
        "    # se guarda el historico de la perdida en el test para graficarlo\n",
        "    loss_history.append(test_loss)\n",
        "\n",
        "    # imprimir por pantalla los resultados del test\n",
        "    print('Test (split ', split,\n",
        "          '):\\n   Loss medio: ', test_loss,\n",
        "          'Perplejidad: ', math.exp(test_loss), '%\\n')\n",
        "\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFO2dTQ05lz4",
        "outputId": "d17dea4b-a0f1-4050-c624-f02111834342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test (split  validation ):\n",
            "   Loss medio:  tensor(4.6563, device='cuda:0') Perplejidad:  105.24189306569585 %\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(4.6563, device='cuda:0')"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ejecutamos un test antes de empezar el entrenamiento\n",
        "model = Transformer()\n",
        "model.to(device)\n",
        "loss_validation_split = []\n",
        "criterion = nn.NLLLoss()\n",
        "eval_iters = 200\n",
        "test('validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NX6bh71360i"
      },
      "source": [
        "# Bucle de entrenamiento\n",
        "\n",
        "En este bloque se inicializa la red y se ejecuta el entrenamiento y test tantas veces como Epochs se hayan configurado.\n",
        "\n",
        "Al completar el entrenamiento se guardan los parametros en un archivo `transformer_model.pt` para que luego pueda leerse para hacer inferencia sin tener que volver a entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lryed8lh7BkK",
        "outputId": "268cc53d-7276-4228-855a-98242481beae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "   Loss:  4.806536674499512\n",
            "   Loss:  2.176502227783203\n",
            "   Loss:  1.8483895063400269\n",
            "   Loss:  1.847395420074463\n",
            "   Loss:  1.7074341773986816\n",
            "   Loss:  1.7307696342468262\n",
            "   Loss:  1.7185612916946411\n",
            "   Loss:  1.6279112100601196\n",
            "   Loss:  1.6207854747772217\n",
            "   Loss:  1.5981956720352173\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  tensor(1.5763, device='cuda:0') Perplejidad:  4.836994072910687 %\n",
            "\n",
            "Epoch: 1\n",
            "   Loss:  1.70144522190094\n",
            "   Loss:  1.4308671951293945\n",
            "   Loss:  1.5377442836761475\n",
            "   Loss:  1.4777427911758423\n",
            "   Loss:  1.5689356327056885\n",
            "   Loss:  1.6485809087753296\n",
            "   Loss:  1.440198540687561\n",
            "   Loss:  1.378949761390686\n",
            "   Loss:  1.3459540605545044\n",
            "   Loss:  1.5847183465957642\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  tensor(1.5058, device='cuda:0') Perplejidad:  4.507577882867828 %\n",
            "\n",
            "Epoch: 2\n",
            "   Loss:  1.4180834293365479\n",
            "   Loss:  1.456786036491394\n",
            "   Loss:  1.3294360637664795\n",
            "   Loss:  1.449589490890503\n",
            "   Loss:  1.4153414964675903\n",
            "   Loss:  1.3768500089645386\n",
            "   Loss:  1.4599882364273071\n",
            "   Loss:  1.4357274770736694\n",
            "   Loss:  1.4584145545959473\n",
            "   Loss:  1.3014074563980103\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  tensor(1.4478, device='cuda:0') Perplejidad:  4.253640163937964 %\n",
            "\n",
            "Epoch: 3\n",
            "   Loss:  1.3116995096206665\n",
            "   Loss:  1.493444800376892\n",
            "   Loss:  1.3410533666610718\n",
            "   Loss:  1.3921194076538086\n",
            "   Loss:  1.3860868215560913\n",
            "   Loss:  1.3320896625518799\n",
            "   Loss:  1.3050333261489868\n",
            "   Loss:  1.4561197757720947\n",
            "   Loss:  1.4638971090316772\n",
            "   Loss:  1.4456192255020142\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  tensor(1.4259, device='cuda:0') Perplejidad:  4.161445257168279 %\n",
            "\n",
            "Epoch: 4\n",
            "   Loss:  1.3958921432495117\n",
            "   Loss:  1.329642415046692\n",
            "   Loss:  1.3754901885986328\n",
            "   Loss:  1.63498055934906\n",
            "   Loss:  1.3906863927841187\n",
            "   Loss:  1.365695834159851\n",
            "   Loss:  1.253433346748352\n",
            "   Loss:  1.5084145069122314\n",
            "   Loss:  1.2668582201004028\n",
            "   Loss:  1.3420475721359253\n",
            "Training Epoch completa\n",
            "\n",
            "Test (split  validation ):\n",
            "   Loss medio:  tensor(1.4180, device='cuda:0') Perplejidad:  4.128759404139384 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# entrenamos la red\n",
        "\n",
        "# semilla del generador de numeros aleatorios\n",
        "# para conseguir resultados deterministas\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# lista para graficar la perdida de entrenamiento y test\n",
        "loss_training = []\n",
        "loss_test_split = []\n",
        "loss_validation_split = []\n",
        "loss_train_split = []\n",
        "\n",
        "# Crear una instancia del modelo y pasarla al dispositivo\n",
        "model = Transformer()\n",
        "model.to(device)\n",
        "\n",
        "# optimizador Adam\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# mejor perdida hasta ahora, por si en alguna iteracion empeoramos\n",
        "best_val_loss = None\n",
        "\n",
        "# cuantos pedazos de texto se usan para entrenar y test en cada epoch\n",
        "train_iters = 5000\n",
        "eval_iters = 200\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(0, epochs):\n",
        "    print('Epoch:', epoch)\n",
        "    train()\n",
        "    val_loss = test('validation')\n",
        "\n",
        "    # guardamos los parametros de la red en un archivo 'transformer_model.pt'\n",
        "    # si val_loss es la mejor que hemos hasta ahora\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        with open('transformer_model.pt', 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_val_loss = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsKDbNlF_LVL",
        "outputId": "fbeb3faa-7438-42e1-e953-09a6b1e72537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test (split  test ):\n",
            "   Loss medio:  tensor(1.4264, device='cuda:0') Perplejidad:  4.163745236912138 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# calculamos la perdida sobre el set de test\n",
        "\n",
        "# cargamos los parametros del modelo de 'transformer_model.pt'\n",
        "with open('transformer_model.pt', 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = test('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgqVpy2F8KZH"
      },
      "source": [
        "# Generacion de texto\n",
        "\n",
        "En este bloque se hace la inferencia con la red ya entrenada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ06iZbXvrRf",
        "outputId": "44bcd97f-da0d-41d1-95b2-aa13e4e74883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "É de milagro cabrón de sus razos, acar de aventura \n",
            "servió y detrás, o don Quijote, como se le semejoles \n",
            "arman; ma que me paesa! ''¡Badas en la razón de las \n",
            "ajes a comer la vi-?\n",
            "— No causar del señor don Quijote, «una fama de nuestra \n",
            "salaldo a decir que quieres, a como seando; que, qué \n",
            "cuando con aventura que por mi alma iba trón —dijo \n",
            "don Quijote—, que le pagará nombre le tabaja; y sol \n",
            "no nos quería dije el casio El cabretis.\n",
            "Por Gábitanten.\n",
            "Con esto, vos descable, según sacaba por el mono delantero, \n",
            "a merced con píaligos que vuesa merced sea paparte \n",
            "a la mesa—abañan verá la fura de esesa—: de la veces \n",
            "más que corrían estaba de una que me caneces? Y su \n",
            "bueno, si ahogaba a pasaz mil jóndano, y a para que \n",
            "haber de las hermosas que aguisor que tenía viejo, \n",
            "le te supa me responden del duques, ni —respondió pero \n",
            "de embazatar en la vencida, importión, barbero que \n",
            "me primero negre y despedido de las naturalezanas, \n",
            "de mi perroganio y Sancho gan . Murestras en yo estamoradas \n",
            "de su hitro, y el cura del cosa cuatire, en quién muy \n",
            "bien cantazónos cabale y malgamientos y la despada \n",
            "de palafijo, segunales, ñerses por lavamos de cabello, \n",
            "sigo lea dellan la grandeza, biste, y mi señor don \n",
            "Quijote la vestera.\n",
            "— No nadiese Olindad —respondió don Quijote——; ha gendo \n",
            "bien casi al delagino puerto de alva, de aquí, en rada, \n",
            "ni bellógemo pesada, y a mi señora.\n",
            "— Segura, faldas —dijo Sancho—.\n",
            "— Y lo sántas de aquella esperada es casa deba de la \n",
            "acertar por verse algo, que detener que el dema, cuanto \n",
            "empresicuradaril son tan desencantado decía.\n",
            "— Ni aquí son, es darle —respondió don Quij los viven \n",
            "a respondiérementes a mi peñería ahecha, ni se acaso \n",
            "sobre un esposoque se paga:\n",
            "Ricendióles, ni al oidor, y me tiendo de los alcas, \n",
            "y te dijese el tache, la digo Entreñara aquel edifeto: \n",
            "que aquí no pienso ellos ha de desberbir a don Quijote \n",
            "de la obligadeza, y más sucedión, y como negocio, junto \n",
            "quiera la sacustarno. Si lo que la canza de su habilitad \n",
            "de algo parar adriencia don Quijote don Quijo"
          ]
        }
      ],
      "source": [
        "# semilla del generador de numeros aleatorios\n",
        "# para conseguir resultados deterministas\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# configuramos como de largo queremos que sea el texto\n",
        "chars = 2000\n",
        "\n",
        "# cargamos los parametros del modelo de 'transformer_model.pt'\n",
        "with open('transformer_model.pt', 'rb') as f:\n",
        "    model = torch.load(f, map_location=device)\n",
        "\n",
        "generate_text()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
